#### 数据分析全景图

数据采集

- 开源数据

    - 单位维度：政府、企业、高校
    - 行业维度：金融、教育、能源

- 爬虫

    - requests + xpath + mysql

- 日志采集

    运维，基于用户访问情况，提升系统的承载能力

    前端埋点，后端脚本采集

    - **埋点就是在你需要统计数据的地方植入统计代码采集用户行为数据**
        - 用户信息、设备信息、操作行为、时间长短等，可使用第三方工具，友盟、Google Analysis、Talkingdata等

    - 采集
        - Web服务器采集，httpd、Nginx、Tomcat自带日志记录
        - 使用 Flume 从不同的服务器集群中采集、汇总和传输大容量的日志数据
        - 分布式框架，Hadoop的Chukwa、Facebook的Scribe
        - 自定义，JS监听用户行为，记录Ajax异步请求日志

- 传感器

数据挖掘

- 数学
    - 概率论与数理统计
    - 线性代数
    - 图论
    - 最优化方法
- 流程
    - 商业理解（数据挖掘目标）
    - 数据探索（部分数据描述、质量验证）
    - 数据准备（收集、清洗、整合）
    - 模型建立
    - 模型评估

数据可视化

- matplotlib、seaborn
- 微图、DataV、Data GIF Maker

#### 数据挖掘十大经典算法

- 分类

    C4.5、朴素贝叶斯、SVN、KNN、Adaboost、CART

- 聚类

    Kmeans、EM

- 关联分析

    Apriori

- 连接分析

    PageRank

#### 为什么Numpy更高效

列表是一个动态的指针数组，存放了每个元素所在的地址。一定范围内的整数存放在整数池中，id不变

- NumPy 数组存储在一个均匀连续的内存块中，遍历所有的元素，不像列表 list 还需要对内存地址进行查找
- 缓存会直接把字节块从 RAM 加载到 CPU 寄存器中。因为数据连续的存储在内存中，NumPy 直接利用现代 CPU 的矢量化指令计算，加载寄存器中的多个连续浮点数
-  NumPy 中的矩阵计算可以采用多线程的方式，充分利用多核 CPU 计算资源

#### 结构化数组

```python
person_type = np.dtype({
    'names': [字段名列表],
    'formats': ['S32', 'i', 'f']  # 格式列表
})

person = np.array([数据列表], dtype=person_type)  # 指定格式
```

#### 数据集成

- ETL

    提取(Extract)——转换(Transform)——加载(Load)，在数据源抽取后首先进行转换，然后将转换的结果写入目的地

- ELT

    提取(Extract)——加载(Load)——变换(Transform)，在抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如Spark来完成转换的步骤

- ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面ELT允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。

- 典型的ETL工具有:

    - 商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services等
    - 开源软件：**Kettle**、Talend、Apatar、Scriptella、DataX、Sqoop等

#### KDD过程

![image-20201007152537797](/Users/yanglu/Library/Application Support/typora-user-images/image-20201007152537797.png)

- 数据预处理**（ETC）**

    - 数据清洗**（C）**

        去重、去噪声、填充缺失值

    - 数据集成**（E）**

        多个数据源/数据表合并

    - 数据变换**（T）**

        归一化

- 数据后处理

    二分类问题得到概率值

你认识了两个漂亮的女孩。

**商业智能**会告诉你要追哪个？成功概率有多大？

**数据仓库**会说，我这里存储了这两个女孩的相关信息，你要吗？

其中每个女孩的数据都有单独的文件夹，里面有她们各自的姓名、生日、喜好和联系方式。这些具体的信息就是**数据元，加起来叫作元数据。**

**数据挖掘**会帮助你确定追哪个女孩，并且整理好数据仓库，这里就可以使用到各种算法，帮你做决策了。

你可能会用到**分类算法**。御姐、萝莉、女王，她到底属于哪个分类？

如果认识的女孩太多了，多到你已经数不过来了，比如说 5 万人！你就可以使用**聚类算法**了，它帮你把这些女孩分成多个群组，比如 5 个组。然后再对每个群组的特性进行了解，进行决策。这样就把 5 万人的决策，转化成了 5 个组的决策。成功实现降维，大大提升了效率。如果你想知道这个女孩的闺蜜是谁，那么**关联分析算法**可以告诉你。

如果你的数据来源比较多，比如有很多朋友给你介绍女朋友，很多人都推荐了同一个，你就需要去重，这叫**数据清洗**；为了方便记忆，你把不同朋友推荐的女孩信息合成一个，这叫**数据集成**；有些数据渠道统计的体重的单位是公斤，有些是斤，你就需要将它们转换成同一个单位，这叫**数据变换。**

最后你可以进行数据可视化了，它会直观地把你想要的结果呈现出来。

```python
# 1、加载数据
# 2、数据探索
pd.set_option('display.max_columns', None)
df.describe()
df.info()
df.head()
# 3、数据清洗
# 4、特征选择
'''PCA降维、corr热力图'''
df_corr = df[feature].corr()
sns.heatmap(df_corr, annot=True)  # annot=True显示每个方格的数据
'''相关性强的几个feature中选一个做代表即可'''
# 5、划分训练集、测试集
# 6、数据标准化变换
# 7、创建模型
# 8、训练、预测、评估
```

#### 数据探索之统计函数

```python
np.amin()、np.max()  # 最大、最小值
np.argmin()、np.argmax()  # 最大、最小值索引
np.ptp()  # 最值之差
np.percentile()  # 百分位数
np.mean()
np.median()
np.average(a,weights=wts)  # 加权平均
np.std()、np.var()
```

#### 数据清洗

数据质量准则：

**完全合一**

- 完整性：空值、空行、是否完善

    ```python
    # 数值型，用mean
    df['Age'].fillna(df['Age'].mean(), inplace=True)
    
    # 类别型，用众数
    age_maxf = train_features['Age'].value_counts().index[0]
    train_features['Age'].fillna(age_maxf, inplace=True)
    
    # 删除空行
    df.drop(how='all', inplace=True)
    ```

- 全面性：看每列数据的mean、min、max，单位是否统一

    ```python
    # 获取 weight 数据列中单位为 lbs 的数据
    rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
    print(df[rows_with_lbs])
    # 将 lbs转换为 kgs, 2.2lbs=1kgs
    for i,lbs_row in df[rows_with_lbs].iterrows():
    	# 截取从头开始到倒数第三个字符之前，即去掉lbs。
    	weight = int(float(lbs_row['weight'][:-3])/2.2)
    	df.at[i,'weight'] = '{}kgs'.format(weight)
    ```

- 合法性：数据类型、数值大小，如非ASCII字符、年龄超过了150岁

    ```python
    # 删除非 ASCII 字符
    df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
    ```

- 唯**一**性：数据拆分、去重

    ```python
    # 拆分
    df[['first_name', 'last_name']] = df['name'].str.split(expand=True)
    # 去重
    df.drop_duplicates()
    ```

```python
df.drop()
df.rename({}, axis=)
df.drop_duplicates()
df['A']=df['A'].map(str.strip)
df.isnull().any()
```

#### 数据变换

- **数据平滑**：去除数据中的噪声，将连续数据离散化，如分箱、聚类和回归

- **数据聚集**：groupby聚合函数

- **数据概化**：将数据由较低的概念抽象成为较高的概念，减少数据复杂度

- **数据规范化**：使属性数据按比例缩放

    - **Min-max 规范化**：新数值=（原数值-极小值）/（极大值-极小值）

    - **Z-Score 规范化**：新数值=（原数值-均值）/ 标准差

    - **小数定标规范化**：通过取值中的最大绝对值的位数移动小数点的位置

        ```python
        # Min-max 规范化
        from sklearn.preprocessing import MinMaxScaler
        MinMaxScaler().fit_transform(x)
        # Z-Score 规范化
        from sklearn.preprocessing import scale, StandardScaler
        scale(x)
        ss = StandardScaler()
        # 小数定标规范化
        j = np.ceil(np.log10(np.max(abs(x))))
        scaled_x = x / (10 ** j)
        ```

        

- **属性构造**：构造出新的属性

#### 数据可视化

![image-20201008234343112](/Users/yanglu/Library/Application Support/typora-user-images/image-20201008234343112.png)

![img](https://static001.geekbang.org/resource/image/bd/5b/bd49dbaffdc170ecc4d56d946afd5c5b.jpg)

![image-20201009000603479](/Users/yanglu/Library/Application Support/typora-user-images/image-20201009000603479.png)

```python
# 散点图
plt.scatter(x, y,marker='x')

df = pd.DataFrame({'x': x, 'y': y})
sns.jointplot(x="x", y="y", data=df, kind='scatter')  # x、y是data中的列名，kind='kde'核密度图，kind='hex'直方图的二维模拟
# 折线图
plt.plot(x, y)  # 提前把数据按照x轴的大小进行排序

df = pd.DataFrame({'x': x, 'y': y})
sns.lineplot(x="x", y="y", data=df)
# 直方图
plt.hist(s)

sns.distplot(s, kde=False)
sns.distplot(s, kde=True)  # 显示核密度估计
# 条形图
plt.bar(x, y)

sns.barplot(x, y)
# 箱线图
plt.boxplot(data,labels=lables)  # labels是每个箱线图的标签

df = pd.DataFrame(data, columns=lables)
sns.boxplot(data=df)
# 饼图
plt.pie(x = nums, labels=labels)
# 热力图
sns.heatmap(data)
# 蜘蛛图
'''显示一对多关系
首先设置两个数组：labels和stats。他们分别保存了这些属性的名称和属性值
计算每个坐标的角度
画完最后一个点后，需要与第一个点进行连线
'''
labels=np.array([u"推进","KDA",u"生存",u"团战",u"发育",u"输出"])
stats=[83, 61, 95, 67, 76, 88]  # 数据准备
angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
stats=np.concatenate((stats,[stats[0]]))
angles=np.concatenate((angles,[angles[0]]))  # 画图数据准备，角度、状态值
fig = plt.figure()
ax = fig.add_subplot(111, polar=True)   
ax.plot(angles, stats, 'o-', linewidth=2)
ax.fill(angles, stats, alpha=0.25)  # 用Matplotlib画蜘蛛图
# 成对关系
sns.pairplot(iris)
```

#### 商业智能BI、数字仓库DW、数据挖掘DM之间的关系

**Business Intelligence**：基于DW，利用DM，得到了商业价值

**Data Warehouse**：将原有的多个数据来源中的数据进行汇总、整理而得，消除数据中的不一致性，方便后续进行数据分析和挖掘

**Data Mining**：分类、聚类、预测、关联分析等

DW是金矿，DM是炼金术，BI分析报告是黄金

数据挖掘也叫 **KDD**（Knowledge Discovery in Database）

#### 用户画像

- 从哪来（**统一化**）

    统一标识ID（用户名、注册手机号、邮箱、设备号、CookieID）

- 都是谁（**标签化**）

    **“用户消费行为分析”**

    - 用户：性别、年龄、地域、收入、学历、职业
- 消费：消费习惯、购买意向、消费均价、等级、是否对促销敏感
    - 行为：时间段、频次、时长、访问路径
    - 内容：停留时间长、浏览次数多的内容，比如，金融、体育、时尚、科技
    
- 要去哪（**业务化**）

    用户画像结合业务，提升转化率、降低留存率

    可以从**用户生命周期**的三个阶段来划分业务价值

    - **获客**：可以找到优势的宣传渠道，通过个性化的宣传手段，吸引有潜在需

        求的用户，并刺激其转化

    - **粘客**：个性化推荐，搜索排序，提升用户的客单价和消费频次，如通过红包、优惠等方式激励对优惠敏感的人群

    - **留客**：流失率预测，分析关键节点降低流失率，用户体验、竞争对手、需求变化

        如果将顾客流失率降低 5%，公司利润将提升 25%~85%

![image-20201007162659137](/Users/yanglu/Library/Application Support/typora-user-images/image-20201007162659137.png)

#### 决策树

基于纯度来构建

信息熵：$Entropy(t) = -\sum_{i=0}^{c-1}p(i|t)*log_2p(i|t)$

经典的 “不纯度”的指标有三种，分别是信息增益（ID3算法）、信息增益率（C4.5算法）以及基尼指数（Cart算法）

- ID3：划分带来纯度提升，信息熵下降，父亲节点的信息熵减去所有子节点的信息熵

    $Gain(D,a) = Entropy(D)-\sum_{i=1}^k\frac{D_i}{D}Entropy(D_i)$

    D：父亲结点，$D_i$：子结点，a：结点的属性选择，$\frac{D_i}{D}$：分支的概率

    缺点：

    - 倾向于选择取值比较多的属性，这样信息增益可能较大。如每一个身份证对应一个人，子结点信息熵=0，信息增益大
    - 对噪声敏感，数据如果有错误，可能会产生决策树分类错误

- C4.5：

    - 采用信息增益率，解决噪声敏感：$信息增益Gain/属性熵H$

        属性熵H：考察属性本身的不确定性，如果这个属性本身不确定性就很大，那就越不倾向于选取它

        $H = -feature取值的概率*log_2(feature取值的概率)$

        当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于C4.5来说，属性熵也会变大，所以整体的信息增益率并不大

    - 采用后剪枝中的悲观剪枝PEP，比较划分前后结点的分类错误率来决定是否划分

    - 可处理连续属性

        **选择具有最高信息增益的划分所对应的阈值**

    - 能处理空缺值

        忽略空缺值所在的行，选取样本D'，计算$Gain/H$

        $Gain/H * \frac{D'样本数}{D样本数}$

    缺点：需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。

- CART：

    - 使用基尼系数来衡量“不纯度”

        当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低

        $Gini(t) = 1-\sum_k[p(C_k|t)^2]$

        结点D被属性a划分后的基尼系数：

        $Gini(D,a)=\sum\frac{D_i}{D}Gini(D_i)$

    - 只支持二叉树，不支持多叉树

    - 分类树

        基于基尼系数来做属性划分

    - 回归树

        样本的离散程度来评价“不纯度”

        $|x-\mu|$：最小绝对偏差(LAD)

        $\frac{1}{n}\sum(x-\mu)^2$：最小二乘偏差(LSD)

        基于每个特征的每个值，二元划分为左右子树，计算左右子树的总方差，找到最小总方差对应的 **feature** 和 **value**

        $总方差=左子树.var() * 左子树样本个数 + 右子树.var() * 右子树样本个数$

    - 剪枝

        使用后剪枝中的CCP(cost-complexity prune)代价复杂度方法

        $\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$，节点t的子树被剪枝后的误差变化除以剪掉的叶子数量

        $t$：根结点，$T_t$：子树，$|T_t|$：子树的叶子数

        $C(t)$：子树被裁剪后的误差，$C(T_t)$：子树被裁剪前的误差

总结：

- ID3算法，基于信息增益 **Gain** 做判断；
- C4.5算法，基于信息增益率 **Gain/H** 做判断；
- CART算法，分类树是基于基尼系数 **Gini** 做判断。回归树是基于偏差 **LAD**、**LSD** 做判断。
    - 最小绝对偏差LAD：$LAD=|x-\mu|$，$\mu$是样本的均值
    - 最小二乘偏差LSD：$LSD=\frac{1}{n}\sum(x-\mu)^2$

#### 朴素贝叶斯

假设每个输入变量是独立的

模型由两种类型的概率组成：1. 每个**类别的概率**P(Cj)；2. 每个属性的**条件概率**P(Ai|Cj)

- 离散数据：

    $P(C_j|A_1A_2A_3)=\frac{P(A_1A_2A_3|C_j)P(C_j)}{P(A_1A_2A_3)}=\frac{P(A_1|C_j)P(A_2|C_j)P(A_3|C_j)P(C_j)}{P(A_1A_2A_3)}$

- 连续数据：

    根据样本，计算均值 $\mu$ 和方差 $\sigma^2$，得到了正态分布密度函数

    把待测数据代入，计算其概率

    ```python
    from scipy.stats import norm
    
    norm.pdf(x, u, sigma)  # Probability density function概率密度函数
    ```

    分类：

    - **高斯朴素贝叶斯**：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。

    - **多项式朴素贝叶斯**：特征变量是离散变量，符合多项分布，以单词为粒度，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的TF-IDF值等。

        ***传入数据不能有负数***，做标准化处理时，使用Min-Max，不使用Z-Score

    - **伯努利朴素贝叶斯**：特征变量是布尔变量，符合0/1分布，以文件为粒度，在文档分类中特征是单词是否出现

    TF-IDF：

    评估某个单词的重要程度

    - **词频TF**：Term Frequency，一个单词在文档中出现的次数，TF越大越重要

        $TF=\frac{单词出现的次数}{文档中包含的总单词数}$

    - **逆向文档频率IDF**：Inverse Document Frequency，单词在文档中的区分度，单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。包含单词的文档数越少，IDF越大，区分度越大

        $IDF=log\frac{文档总数}{包含单词的文档数+1}$，+1避免分母为0

    $TF-IDF=TF*IDF$

    ```python
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    tfidf_vec = TfidfVectorizer(stop_words=停用词列表, token_pattern=过滤规则)
    tfidf_matrix = tfidf_vec.fit_transform(文档列表)
    # 输出文档中所有不重复的词
    tfidf_vec.get_feature_names()
    # 输出每个单词在词汇表中的id值
    tfidf_vec.vocabulary_
    # 输出每个单词在每个文档中的TF-IDF值
    tfidf_matrix.toarray()
    ```

    ![img](https://static001.geekbang.org/resource/image/25/c3/257e01f173e8bc78b37b71b2358ff7c3.jpg)

流程：

- **分词**

    ```python
    # 英文用nltk
    import nltk
    word_list = nltk.word_tokenize(text) #分词
    nltk.pos_tag(word_list) #标注单词的词性
    
    # 中文用jieba
    import jieba
    word_list = jieba.cut(text) #中文分词
    ```

- **加载停用词**

    从网上可以找到中文常用的停用词保存在stop_words.txt

    ```python
    stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readlines()]
    ```

- **计算单词的权重**

    ```python
    tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)  # max_df=0.5，代表一个单词在50%的文档中都出现过了，则忽略
    ```

- **生成朴素贝叶斯分类器**

    ```python
    # 多项式贝叶斯分类器
    from sklearn.naive_bayes import MultinomialNB  
    clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
    # alpha为平滑参数
    '''如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为0。为了解决这个问题，我们需要做平滑处理。'''
    # 当alpha=1时，使用的是Laplace平滑。Laplace平滑就是采用加1的方式，来统计没有出现过的单词的概率
    # 当0<alpha<1时，使用的是Lidstone平滑。对于Lidstone平滑来说，alpha 越小，迭代次数越多，精度越高
    ```

- **使用生成的分类器做预测**

    ```python
    test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)
    test_features=test_tf.fit_transform(test_contents)
    predicted_labels=clf.predict(test_features)
    ```

#### SVM

寻找最大化margin的超平面的过程

- 硬间隔

    数据是完全的线性可分的

- 软间隔

    数据存在一些噪点，近似线性可分

- 核函数

    非线性数据，**将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分**

多分类问题

​	分成A、B、C三种分类

- 一对多

    A作为正集，B、C作为负集

    B作为正集，A、C作为负集

    C作为正集，A、B作为负集

- 一对一

    k个分类，构造成$C_k^2$个分类器，如3个分类有3个分类器

    分类器1：A、B

    分类器2：A、C

    分类器3：B、C

    综合所有分类器的结果进行投票

SVM既可以做回归SVR，也可以做分类SVC

分类：

- SVC

    不要求数据线性可分

    kernel可选

    - linear：线性核函数，数据线性可分时使用
    - poly：多项式核函数，参数多，计算大
    - rbf：高斯核函数（默认），参数少
    - sigmoid：sigmoid核函数，映射多层神经网络

    C：

    惩罚系数，C越大，越容易过拟合

    gamma：

    核函数系数，默认 $\gamma=\frac{1}{特征维度n}$

- LinearSVC

    要求数据线性可分，kernel为线性核函数

#### KNN

1. 计算待分类物体与其他物体之间的距离；
2. 统计距离最近的K个邻居；
3. 对于K个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类

- k值

    使用交叉验证，来确定k值

    - **k值小**，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，**过拟合**
    - **k值大**，距离过远的点也会对未知物体的分类产生影响，**欠拟合**

- 距离计算

    - **欧式距离**：$d=\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$

    - **曼哈顿距离**：$d=|x_1-x_2|+|y_1-y_2|+\dotsm+|z_1-z_2|$

    - **切比雪夫距离**：$d=max(|x_1-x_2|,|y_1-y_2|,\dotsm,|z_1-z_2|)$

    - **闵可夫斯基距离**：是一组距离的定义

        $d=\sqrt[P]{\sum_{i=1}^n(x_i-y_i)^P}$

        p=1：曼哈顿距离

        p=2：欧式距离

        p=$\infin$：切比雪夫距离

    - **余弦距离**：在方向上比较两个向量之间的差距。如比较兴趣相关性

        $cos\theta=\frac{\vec{x}*\vec{y}}{\rVert\vec{x}\rVert*\rVert\vec{y}\rVert}$

- KD树

    KNN需要大量计算样本点之间的距离，KD树的每个节点都是k维数值点的二叉树，使用KD树存储数据，大大提升搜索效率

```python
from sklearn.neighbors import KNeighborsClassifier

KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30)
# weights：是用来确定邻居的权重，有三种方式：
'''
1、weights=uniform，代表所有邻居的权重相同；
2、weights=distance，代表权重是距离的倒数，即与距离成反比；
3、自定义函数'''
# algorithm：用来规定计算邻居的方法，它有四种方式:
'''
1、algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择auto；
2、algorithm=kd_tree，也叫作KD树，是多维空间的数据结构，方便对关键数据进行检索，不过KD树适用于维度少的情况，一般维数不超过20，如果维数大于20之后，效率反而会下降；
3、algorithm=ball_tree，也叫作球树，不同于KD树，球树更适用于维度大的情况；
4、algorithm=brute，也叫作暴力搜索，不构造树。当训练集大的时候，效率很低。'''
# leaf_size：代表构造KD树或球树时的叶子数，默认是30，调整leaf_size会影响到树的构造和搜索速度
```

#### Kmeans

1. 选取K个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的；
2. 将每个点分配到最近的类中心点，这样就形成了K个类，然后重新计算每个类的中心点；
3. 重复第二步，直到类不发生变化，或者设置最大迭代次数

![image-20201009232608451](/Users/yanglu/Library/Application Support/typora-user-images/image-20201009232608451.png)

```python
from sklearn.cluster import KMeans

KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001)
# n_init：初始化中心点的运算次数，默认是10。
'''程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行n_init次, 取其中最好的作为初始的中心点。如果K值比较大的时候，你可以适当增大n_init这个值'''
# init： 即初始值选择的方式，默认是采用优化过的k-means++方式，也可使用'random'
```

#### EM聚类

初始化参数——>观察预期——>重新估计